{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T21:18:25.979240Z",
     "start_time": "2025-03-19T21:18:23.909759Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tests.estimators.classification.test_jax import classifier\n",
    "from torch.nn.functional import batch_norm\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import ssl\n",
    "import random\n",
    "from pathlib import Path\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_stdlib_context"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giorgos/PycharmProjects/defensive_distillation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f4d59360ec742626"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:18:30.281549Z",
     "start_time": "2025-03-19T21:18:30.275425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure reproducibility\n",
    "\n",
    "# Set a global random seed\n",
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "id": "621627dc3f13e3eb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:18:31.840361Z",
     "start_time": "2025-03-19T21:18:31.837825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Teacher Model\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "n_channels, w, h = 1, 28, 28\n",
    "max_epochs = 5"
   ],
   "id": "726faf7d9df4ae92",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:18:39.120254Z",
     "start_time": "2025-03-19T21:18:39.117841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Attacks\n",
    "max_attack_iter = 50"
   ],
   "id": "1eefce045dc1b249",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:18:49.245710Z",
     "start_time": "2025-03-19T21:18:49.217336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dt_p = Path('data/mnist')\n",
    "\n",
    "classes = [str(i) for i in range(10)]\n",
    "n_labels = len(classes)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "trainset = datasets.MNIST(root=f'{dt_p.absolute()}/train', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "testset = datasets.MNIST(root=f'{dt_p.absolute()}/test', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ],
   "id": "6e08b96d6621e9ee",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:18:52.275006Z",
     "start_time": "2025-03-19T21:18:52.245905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MnistNet(nn.Module):\n",
    "    def __init__(self, input_size=28):  # Add input size for flexibility\n",
    "        super().__init__()\n",
    "        self.conv1a = nn.Conv2d(1, 32, 3, padding=0)\n",
    "        self.conv1b = nn.Conv2d(32, 32, 3, padding=0)\n",
    "        self.conv2a = nn.Conv2d(32, 64, 3, padding=0)\n",
    "        self.conv2b = nn.Conv2d(64, 64, 3, padding=0)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Compute the number of features dynamically\n",
    "        self._to_linear = self._get_conv_output(input_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.batchnorm = nn.BatchNorm1d(200)  # Now correctly applied\n",
    "\n",
    "    def _get_conv_output(self, size):\n",
    "        \"\"\"Helper function to compute the output size after convolutions\"\"\"\n",
    "        x = torch.zeros(1, 1, size, size)  # Create a dummy tensor\n",
    "        x = self.pool(F.relu(self.conv1b(F.relu(self.conv1a(x)))))\n",
    "        x = self.pool(F.relu(self.conv2b(F.relu(self.conv2a(x)))))\n",
    "        return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1a(x))\n",
    "        x = self.activation(self.conv1b(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.activation(self.conv2a(x))\n",
    "        x = self.activation(self.conv2b(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.flatten(x)  # Flatten before FC layers\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.fc2(x)\n",
    "        # CrossEntropyLoss already applied softmax\n",
    "        return x  # Remove softmax if using CrossEntropyLoss\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Playing on {device}\")"
   ],
   "id": "7140eba58d6b46ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing on mps\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:18:56.485080Z",
     "start_time": "2025-03-19T21:18:56.400270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify teacher model\n",
    "\n",
    "teacher_model = MnistNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(teacher_model.parameters(), lr=lr)"
   ],
   "id": "c4aa920188d3a57f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:19:26.051318Z",
     "start_time": "2025-03-19T21:19:06.378585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the teacher model\n",
    "teacher_losses = []\n",
    "\n",
    "for e in tqdm(range(max_epochs)):\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # Forward pass\n",
    "        logits = teacher_model(images)\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    teacher_losses.append(loss.item())\n",
    "\n",
    "    if e % 10 == 0 or e == max_epochs:\n",
    "        print(f\"Epoch {e}: {loss.item()}\")"
   ],
   "id": "533bdc1f87d692f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:07<00:28,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.1005193293094635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:19<00:00,  3.93s/it]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(model, data, labels, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy and loss.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model (ART-wrapped or regular)\n",
    "        data: Input samples (NumPy array)\n",
    "        labels: True labels (NumPy array, one-hot encoded)\n",
    "        criterion: Loss function (e.g., nn.CrossEntropyLoss())\n",
    "        device: \"cpu\" or \"cuda\" (use GPU if available)\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float)\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Convert NumPy data to PyTorch tensors\n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Forward pass: Compute predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data_tensor)  # Get raw logits\n",
    "        predictions = torch.argmax(outputs, dim=1)  # Convert to class labels\n",
    "        true_labels = torch.argmax(labels_tensor, dim=1)  # Convert one-hot to labels\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = (predictions == true_labels).float().mean().item() * 100\n",
    "\n",
    "    return accuracy"
   ],
   "id": "4d74ef7d74ab1b50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:19:28.558749Z",
     "start_time": "2025-03-19T21:19:28.550189Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 10,
   "source": [
    "# Wrap in ART PyTorchClassifier\n",
    "art_model_t = PyTorchClassifier(\n",
    "    model=teacher_model,\n",
    "    clip_values=(0, 1),  # Min and Max pixel values (normalize if needed)\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(1, 28, 28),\n",
    "    nb_classes=10\n",
    ")\n",
    "art_model_t.to(device)  # Move model to CPU"
   ],
   "id": "4f7f74004b70a472"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:19:32.484708Z",
     "start_time": "2025-03-19T21:19:32.479199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist_targets = torch.nn.functional.one_hot(testset.targets, num_classes=10).float().numpy()\n",
    "mnist_data = testset.data.unsqueeze(1).float().numpy()  # Add channel dimension (N, 1, 28, 28)"
   ],
   "id": "3a3833899cd89f47",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:38:26.667965Z",
     "start_time": "2025-03-19T21:38:26.658395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Computationally Expensive methods -> Select Test Subset\n",
    "num_samples = 200\n",
    "# First shuffle\n",
    "indices = torch.randperm(len(testset.data))\n",
    "mnist_data_shuffled = mnist_data[indices]\n",
    "mnist_targets_shuffled = mnist_targets[indices]\n",
    "# Then select subsets\n",
    "mnist_data_subset = mnist_data_shuffled[:num_samples]\n",
    "mnist_targets_subset = mnist_targets_shuffled[:num_samples]"
   ],
   "id": "f21848242c843dac",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ensure teacher model is on CPU to create the attacks\n",
    "teacher_model.to('cpu')"
   ],
   "id": "5b990fefe42d48b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T22:25:41.078681Z",
     "start_time": "2025-03-19T22:25:40.613387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Attack #1\n",
    "from art.attacks.evasion.fast_gradient import FastGradientMethod\n",
    "\n",
    "attack = FastGradientMethod(estimator=art_model_t, eps=0.5, eps_step=0.1, batch_size=32, minimal=True)\n",
    "x_adv_fgm = attack.generate(x=mnist_data_subset, y=mnist_targets_subset)"
   ],
   "id": "22434a8e7cd1445f",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:38:44.111704Z",
     "start_time": "2025-03-19T21:38:28.378270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Attack #2\n",
    "\n",
    "from art.attacks.evasion.deepfool import DeepFool\n",
    "\n",
    "attack = DeepFool(classifier=art_model_t, max_iter=max_attack_iter, batch_size=32)\n",
    "x_adv_deepfool = attack.generate(x=mnist_data_subset, y=mnist_targets_subset)"
   ],
   "id": "3872b18a7d8a4eda",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeepFool: 100%|██████████| 7/7 [00:15<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:56:07.699523Z",
     "start_time": "2025-03-19T21:43:23.123867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Attack #3\n",
    "\n",
    "from art.attacks.evasion.carlini import CarliniL2Method\n",
    "\n",
    "attack = CarliniL2Method(classifier=art_model_t, max_iter=max_attack_iter, batch_size=32)\n",
    "x_adv_carlini = attack.generate(x=mnist_data_subset, y=mnist_targets_subset)"
   ],
   "id": "41a42687390e23d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C&W L_2: 100%|██████████| 7/7 [12:44<00:00, 109.21s/it]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T22:25:44.267202Z",
     "start_time": "2025-03-19T22:25:44.171728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Teacher Model\n",
    "\n",
    "# Original test set evaluation\n",
    "original_accuracy = evaluate_model(\n",
    "    art_model_t.model, mnist_data_subset, mnist_targets_subset, criterion\n",
    ")\n",
    "print(f\"Original Test Accuracy: {original_accuracy:.2f}%\")\n",
    "\n",
    "# Adversarial test sets evaluation\n",
    "fgm_accuracy = evaluate_model(\n",
    "    art_model_t.model, x_adv_fgm, mnist_targets_subset, criterion\n",
    ")\n",
    "deepfool_accuracy = evaluate_model(\n",
    "    art_model_t.model, x_adv_deepfool, mnist_targets_subset, criterion\n",
    ")\n",
    "carlini_accuracy = evaluate_model(\n",
    "    art_model_t.model, x_adv_carlini, mnist_targets_subset, criterion\n",
    ")\n",
    "\n",
    "print(f\"Adversarial Test Accuracy (FGS Method): {fgm_accuracy:.2f}%\")\n",
    "print(f\"Adversarial Test Accuracy (Deepfool Method): {deepfool_accuracy:.2f}%\")\n",
    "print(f\"Adversarial Test Accuracy (Carlini L2 Method): {carlini_accuracy:.2f}%\")"
   ],
   "id": "88d9eec0f78f1f5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test Accuracy: 90.50%\n",
      "Adversarial Test Accuracy (FGS Method): 91.00%\n",
      "Adversarial Test Accuracy (Deepfool Method): 7.00%\n",
      "Adversarial Test Accuracy (Carlini L2 Method): 90.50%\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T22:25:48.830156Z",
     "start_time": "2025-03-19T22:25:48.828189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate Soft Labels\n",
    "\n",
    "def get_soft_labels(model, dataloader, temp):\n",
    "    soft_labels_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            # Get logits from trained teacher model\n",
    "            logits = model(images)\n",
    "            # Apply temperature-scaled softmax\n",
    "            soft_labels = torch.softmax(logits / temp, dim=1)\n",
    "            soft_labels_list.append(soft_labels.cpu())\n",
    "    return torch.cat(soft_labels_list)"
   ],
   "id": "c5722c14f39e829d",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T22:25:50.200764Z",
     "start_time": "2025-03-19T22:25:50.198205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def distillation_loss(student_logits, teacher_soft_labels, hard_labels, temp, alpha):\n",
    "    \"\"\"\n",
    "    Computes the distillation loss.\n",
    "\n",
    "    Args:\n",
    "        student_logits: Output logits from the student model.\n",
    "        teacher_soft_labels: Soft labels generated by the teacher model.\n",
    "        hard_labels: Original hard labels.\n",
    "        temp: Temperature used for soft labels.\n",
    "        alpha: Weight for soft loss (1-alpha for hard loss).\n",
    "    \"\"\"\n",
    "    soft_loss = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "        torch.log_softmax(student_logits / temp, dim=1),\n",
    "        teacher_soft_labels\n",
    "    )\n",
    "    hard_loss = nn.CrossEntropyLoss()(student_logits, hard_labels)\n",
    "    return alpha * soft_loss + (1 - alpha) * hard_loss\n"
   ],
   "id": "6ade93929ca6447f",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:57:26.833438Z",
     "start_time": "2025-03-19T21:57:26.829766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_student(teacher_model, student_model, trainloader, temp=20, alpha=0.7, epochs=10, lr=0.01):\n",
    "    \"\"\"\n",
    "    Trains the student model using knowledge distillation.\n",
    "\n",
    "    Args:\n",
    "        teacher_model: Pretrained teacher model.\n",
    "        student_model: Student model to train.\n",
    "        trainloader: DataLoader for training.\n",
    "        temp: Temperature for soft labels.\n",
    "        alpha: Weight for soft labels in the loss.\n",
    "        epochs: Number of training epochs.\n",
    "        lr: Learning rate.\n",
    "    \"\"\"\n",
    "    student_model.train()\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=lr)\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for images, hard_labels in trainloader:\n",
    "            images, hard_labels = images.to(device), hard_labels.to(device)\n",
    "\n",
    "            # Get teacher soft labels\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(images)\n",
    "                soft_labels = torch.softmax(teacher_logits / temp, dim=1)\n",
    "\n",
    "            # Get student predictions\n",
    "            student_logits = student_model(images)\n",
    "\n",
    "            # Compute distillation loss\n",
    "            loss = distillation_loss(student_logits, soft_labels, hard_labels, temp, alpha)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if e % 10 == 0:\n",
    "            print(f\"Epoch {e}: {loss.item()}\")\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ],
   "id": "8bab45804f1004e7",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T22:03:16.284562Z",
     "start_time": "2025-03-19T22:03:16.274076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify Student Model\n",
    "\n",
    "temp = 20\n",
    "max_epochs = 10\n",
    "alpha = 0.5\n",
    "# Initialize the student model\n",
    "student_model = MnistNet().to(device)"
   ],
   "id": "916788032adeb49a",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T22:03:20.519414Z",
     "start_time": "2025-03-19T22:03:18.396228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "teacher_model.to(device)\n",
    "# Generate soft labels for training the student\n",
    "soft_labels = get_soft_labels(teacher_model, trainloader, temp)"
   ],
   "id": "1da8e9013510b1be",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T22:04:07.793538Z",
     "start_time": "2025-03-19T22:03:22.291618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Student\n",
    "train_student(teacher_model, student_model, trainloader, temp=temp, alpha=alpha, epochs=max_epochs, lr=lr)"
   ],
   "id": "672d92a8ca4f3d8b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:04<00:40,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.020531142130494118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T22:04:41.684837Z",
     "start_time": "2025-03-19T22:04:41.671022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Wrap in ART PyTorchClassifier\n",
    "art_model_s = PyTorchClassifier(\n",
    "    model=student_model,\n",
    "    clip_values=(0, 1),  # Min and Max pixel values (normalize if needed)\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(1, 28, 28),\n",
    "    nb_classes=10\n",
    ")"
   ],
   "id": "48bf154e1772c49e",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T22:26:35.489476Z",
     "start_time": "2025-03-19T22:26:35.392336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Student Model\n",
    "\n",
    "# Original test set evaluation\n",
    "original_accuracy = evaluate_model(\n",
    "    art_model_s.model, mnist_data_subset, mnist_targets_subset, criterion\n",
    ")\n",
    "print(f\"Original Test Accuracy: {original_accuracy:.2f}%\")\n",
    "\n",
    "# Adversarial test sets evaluation\n",
    "\n",
    "fgm_accuracy = evaluate_model(\n",
    "    art_model_s.model, x_adv_fgm, mnist_targets_subset, criterion\n",
    ")\n",
    "deepfool_accuracy = evaluate_model(\n",
    "    art_model_s.model, x_adv_deepfool, mnist_targets_subset, criterion\n",
    ")\n",
    "carlini_accuracy = evaluate_model(\n",
    "    art_model_s.model, x_adv_carlini, mnist_targets_subset, criterion\n",
    ")\n",
    "\n",
    "print(f\"Adversarial Test Accuracy (FGS Method): {fgm_accuracy:.2f}%\")\n",
    "print(f\"Adversarial Test Accuracy (Deepfool Method): {deepfool_accuracy:.2f}%\")\n",
    "print(f\"Adversarial Test Accuracy (Carlini L2 Method): {carlini_accuracy:.2f}%\")"
   ],
   "id": "789fae96d22d774e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test Accuracy: 99.50%\n",
      "Adversarial Test Accuracy (FGS Method): 91.00%\n",
      "Adversarial Test Accuracy (Deepfool Method): 11.50%\n",
      "Adversarial Test Accuracy (Carlini L2 Method): 99.50%\n"
     ]
    }
   ],
   "execution_count": 86
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
